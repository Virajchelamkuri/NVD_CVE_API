# -*- coding: utf-8 -*-
"""Injestion code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGevTMWZikvaMRB8IUfmFIldRQvZub1T
"""

import requests
import sqlite3
import time
from datetime import datetime

# ================================
# Database Setup (SQLite in Colab)
# ================================
DB_FILE = "cves.db"

conn = sqlite3.connect(DB_FILE)
cur = conn.cursor()

cur.execute("""
CREATE TABLE IF NOT EXISTS cves (
    cve_id TEXT PRIMARY KEY,
    published_date TEXT,
    last_modified TEXT,
    base_score_v3 REAL,
    base_score_v2 REAL,
    description TEXT,
    raw_json TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
)
""")
conn.commit()


# ================================
# Helper functions
# ================================
BASE_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"

def fetch_page(start_index=0, results_per_page=2000):
    """Fetch one page of CVEs from NVD API"""
    params = {
        "startIndex": start_index,
        "resultsPerPage": results_per_page
    }
    r = requests.get(BASE_URL, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def extract_fields(item):
    """Extract key fields from CVE JSON item"""
    cve_id = item.get("id")

    published = item.get("published")
    last_modified = item.get("lastModified")

    # description (english only if possible)
    desc = None
    for d in item.get("descriptions", []):
        if d.get("lang") == "en":
            desc = d.get("value")
            break

    # CVSS scores
    score_v3 = None
    score_v2 = None

    metrics = item.get("metrics", {})
    if "cvssMetricV3" in metrics:
        score_v3 = metrics["cvssMetricV3"][0]["cvssData"].get("baseScore")
    if "cvssMetricV2" in metrics:
        score_v2 = metrics["cvssMetricV2"][0]["cvssData"].get("baseScore")

    return (cve_id, published, last_modified, score_v3, score_v2, desc, str(item))


def upsert_cve(cve):
    """Insert or update a CVE in SQLite"""
    cur.execute("""
    INSERT INTO cves (cve_id, published_date, last_modified, base_score_v3, base_score_v2, description, raw_json, updated_at)
    VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
    ON CONFLICT(cve_id) DO UPDATE SET
        published_date=excluded.published_date,
        last_modified=excluded.last_modified,
        base_score_v3=excluded.base_score_v3,
        base_score_v2=excluded.base_score_v2,
        description=excluded.description,
        raw_json=excluded.raw_json,
        updated_at=CURRENT_TIMESTAMP
    """, cve)
    conn.commit()


# ================================
# Main Ingestion Loop
# ================================
def full_sync(max_pages=3, page_size=2000):
    """
    Fetch CVEs from NVD API and store in DB.
    max_pages: limit pages for demo (remove for full sync)
    """
    start = 0
    page = 0
    total_inserted = 0

    while True:
        print(f"Fetching page {page+1} (startIndex={start})...")
        data = fetch_page(start_index=start, results_per_page=page_size)

        vulns = data.get("vulnerabilities", [])
        if not vulns:
            print("No more results, stopping.")
            break

        for v in vulns:
            item = v.get("cve")
            if item:
                cve_data = extract_fields(item)
                upsert_cve(cve_data)
                total_inserted += 1

        start += len(vulns)
        page += 1

        # demo stop after limited pages
        if page >= max_pages:
            print("Reached demo page limit.")
            break

        # be polite to API
        time.sleep(0.6)

    print(f"✅ Sync completed. Inserted/Updated {total_inserted} CVEs.")


# ================================
# Run Ingestion
# ================================
full_sync(max_pages=2, page_size=1000)  # adjust/remove limits for full sync

# Check how many CVEs got inserted
cur.execute("SELECT COUNT(*) FROM cves")
print("Total CVEs stored:", cur.fetchone()[0])

# Preview first 5 rows
import pandas as pd
df = pd.read_sql_query("SELECT * FROM cves LIMIT 5", conn)
df

# Install dependencies (run once in Colab)
!pip install --quiet psycopg2-binary requests

!pip install psycopg2-binary requests tqdm

import psycopg2
import psycopg2.extras
import requests
import time
import random
from datetime import datetime, timezone
from tqdm import tqdm

# ==============================
# 1. Neon DB Connection
# ==============================
DB_CONFIG = {
    "dbname": "neondb",
    "user": "neondb_owner",
    "password": "npg_C0rMuPzQB5qd",
    "host": "ep-round-voice-a1cdpuk5-pooler.ap-southeast-1.aws.neon.tech",
    "port": "5432",
    "sslmode": "require"
}

conn = psycopg2.connect(**DB_CONFIG)
cur = conn.cursor()

# ==============================
# 2. Create Tables
# ==============================
cur.execute("""
CREATE TABLE IF NOT EXISTS cves (
    cve_id TEXT PRIMARY KEY,
    published_date TIMESTAMP,
    last_modified TIMESTAMP,
    base_score_v3 REAL,
    base_score_v2 REAL,
    description TEXT,
    raw_json JSONB,
    created_at TIMESTAMP DEFAULT now(),
    updated_at TIMESTAMP DEFAULT now()
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS sync_state (
    id TEXT PRIMARY KEY,
    last_start_index BIGINT,
    last_sync_time TIMESTAMP WITH TIME ZONE,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
""")
conn.commit()

# ==============================
# 3. Helper Functions
# ==============================
BASE_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"

def fetch_page(start_index=0, results_per_page=1000):
    params = {
        "startIndex": start_index,
        "resultsPerPage": results_per_page
    }
    r = requests.get(BASE_URL, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def extract_fields(item):
    cve_id = item.get("id")
    published = item.get("published")
    last_modified = item.get("lastModified")

    desc = None
    for d in item.get("descriptions", []):
        if d.get("lang") == "en":
            desc = d.get("value")
            break

    score_v3, score_v2 = None, None
    metrics = item.get("metrics", {})
    if "cvssMetricV3" in metrics:
        score_v3 = metrics["cvssMetricV3"][0]["cvssData"].get("baseScore")
    if "cvssMetricV2" in metrics:
        score_v2 = metrics["cvssMetricV2"][0]["cvssData"].get("baseScore")

    return {
        "cve_id": cve_id,
        "published_date": published,
        "last_modified": last_modified,
        "base_score_v3": score_v3,
        "base_score_v2": score_v2,
        "description": desc,
        "raw_json": item
    }

def upsert_cve_batch(cves):
    # Convert raw_json dict to JSON for psycopg2
    for cve in cves:
        cve['raw_json'] = psycopg2.extras.Json(cve['raw_json'])

    cur.executemany("""
        INSERT INTO cves (cve_id, published_date, last_modified, base_score_v3, base_score_v2, description, raw_json, updated_at)
        VALUES (%(cve_id)s, %(published_date)s, %(last_modified)s, %(base_score_v3)s, %(base_score_v2)s, %(description)s, %(raw_json)s, now())
        ON CONFLICT (cve_id) DO UPDATE SET
            published_date = EXCLUDED.published_date,
            last_modified = EXCLUDED.last_modified,
            base_score_v3 = EXCLUDED.base_score_v3,
            base_score_v2 = EXCLUDED.base_score_v2,
            description = EXCLUDED.description,
            raw_json = EXCLUDED.raw_json,
            updated_at = now()
    """, cves)
    conn.commit()

def save_checkpoint(start_index):
    cur.execute("""
        INSERT INTO sync_state (id, last_start_index, updated_at)
        VALUES ('nvd_cve', %s, now())
        ON CONFLICT (id) DO UPDATE
        SET last_start_index = EXCLUDED.last_start_index,
            updated_at = now();
    """, (start_index,))
    conn.commit()

def load_checkpoint():
    cur.execute("SELECT last_start_index FROM sync_state WHERE id='nvd_cve'")
    row = cur.fetchone()
    return row[0] if row else 0

# ==============================
# 4. Ingestion Loop
# ==============================
def run_ingestion(limit=50000, page_size=1000, delay=1.0):
    start_index = load_checkpoint()
    inserted = 0

    while inserted < limit:
        print(f"Fetching CVEs from index {start_index}...")
        try:
            data = fetch_page(start_index=start_index, results_per_page=page_size)
        except Exception as e:
            print(f"Error fetching data: {e}, retrying in 5s...")
            time.sleep(5)
            continue

        vulns = data.get("vulnerabilities", [])
        if not vulns:
            print("No more data. Stopping.")
            break

        batch = []
        for v in vulns:
            item = v.get("cve")
            if not item:
                continue
            batch.append(extract_fields(item))

        if batch:
            upsert_cve_batch(batch)
            inserted += len(batch)
            print(f"Inserted {inserted} CVEs so far...")

        start_index += len(vulns)
        save_checkpoint(start_index)

        if inserted >= limit:
            print(f"Reached session limit of {limit} CVEs.")
            break

        time.sleep(delay)

    print(f"✅ Ingestion session completed. Total inserted/updated this run: {inserted}")


# ==============================
# 5. Run Script
# ==============================
run_ingestion(limit=100000, page_size=1000, delay=1.0)